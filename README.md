# ğŸ¤– Chatbot RAG avec LLM

Un chatbot intelligent construit avec **FastAPI**, utilisant la technique **RAG (Retrieval-Augmented Generation)** combinant la recherche sÃ©mantique et la gÃ©nÃ©ration de texte par LLM (Large Language Model).

## âœ¨ FonctionnalitÃ©s

- ğŸ” **Retrieval Engine** : Recherche sÃ©mantique dans la FAQ avec SentenceTransformers
- ğŸ§  **LLM Generation** : GÃ©nÃ©ration de rÃ©ponses naturelles avec Groq LLM (gratuit)
- ğŸ’¾ **Memory Management** : Gestion de l'historique de conversation par utilisateur
- ğŸŒ **Interface Web** : Interface web moderne et conviviale
- ğŸ“Š **Hybrid Matching** : Combinaison de recherche dans les questions et rÃ©ponses
- âš¡ **Caching** : Cache des embeddings pour optimiser les performances
- ğŸ”„ **FAQ Dynamique** : Sauvegarde automatique des nouvelles questions et gestion
- ğŸ¯ **Confidence Scoring** : Ã‰valuation de la fiabilitÃ© des rÃ©ponses

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ (Interface Web)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI    â”‚ (main.py)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º RetrievalEngine â”€â”€â–º SentenceTransformer â”€â”€â–º Base de donnÃ©es FAQ
       â”‚
       â”œâ”€â”€â–º MemoryManager â”€â”€â–º Historique de conversation
       â”‚
       â””â”€â”€â–º LLMManager â”€â”€â–º API Groq
```

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- pip
- ClÃ© API depuis [Groq Console](https://console.groq.com) (gratuit)

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone <repository-url>
cd ChatBot
```

### 2. CrÃ©er un environnement virtuel

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. Configurer la clÃ© API

CrÃ©er un fichier `.env` Ã  la racine :

```bash
# Copier depuis .env.example
cp .env.example .env
```

Ou crÃ©er manuellement et ajouter :

```env
GROQ_API_KEY=votre_clÃ©_api_ici
```

**Obtenir une clÃ© API :**
1. S'inscrire sur [Groq Console](https://console.groq.com)
2. Aller dans **API Keys** dans le tableau de bord
3. Copier la clÃ© et la coller dans le fichier `.env`

### 5. Lancer l'application

```bash
python main.py
```

Ou utiliser uvicorn directement :

```bash
uvicorn main:app --reload --host 127.0.0.1 --port 8000
```

AccÃ©der Ã  : http://127.0.0.1:8000

## ğŸ“ Structure du projet

```
ChatBot/
â”œâ”€â”€ main.py                 # Application FastAPI principale
â”œâ”€â”€ llm_manager.py          # Gestion des providers LLM (Groq)
â”œâ”€â”€ retrieval_engine.py     # Moteur de recherche sÃ©mantique
â”œâ”€â”€ memory_manager.py       # Gestion de l'historique de conversation
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ .env.example           # ModÃ¨le pour les variables d'environnement
â”œâ”€â”€ README.md              # Ce fichier
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ faq.json          # Base de donnÃ©es FAQ
â”‚   â””â”€â”€ new_questions.json # Questions nouvelles non encore ajoutÃ©es Ã  la FAQ
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Interface web
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ chatbot.css   # Styles pour le chatbot
â”‚   â”‚   â””â”€â”€ theme.css     # ThÃ¨me gÃ©nÃ©ral
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ chatbot.js    # JavaScript pour le frontend
â”‚
â””â”€â”€ test/
    â””â”€â”€ test_retrieval.py # Tests unitaires
```

## ğŸ”§ Configuration

### Changer le modÃ¨le d'embedding

Dans `main.py`, vous pouvez changer le modÃ¨le SentenceTransformer :

```python
retriever = RetrievalEngine(
    faq_path="data/faq.json",
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
```

ModÃ¨les populaires :
- `paraphrase-multilingual-MiniLM-L12-v2` (multilingue, rapide)
- `all-MiniLM-L6-v2` (anglais, trÃ¨s rapide)
- `paraphrase-multilingual-mpnet-base-v2` (multilingue, plus prÃ©cis mais plus lent)

### Changer le provider LLM

Par dÃ©faut utilise Groq. Pour changer dans `main.py` :

```python
llm_manager = LLMManager(preferred_provider="groq")
```

### Ajuster le seuil (Threshold)

Dans `retrieval_engine.py`, la fonction `get_best_match()` a un paramÃ¨tre `threshold` (par dÃ©faut 0.45) :
- **Plus Ã©levÃ©** (0.6+) : Retourne uniquement les rÃ©sultats trÃ¨s certains, plus de questions utiliseront le LLM
- **Plus bas** (0.3-) : Retourne plus de rÃ©sultats, moins d'utilisation du LLM

## ğŸ“¡ Points d'accÃ¨s API

### Endpoints Chat

- `POST /chat` - Envoyer un message et recevoir une rÃ©ponse
  ```json
  {
    "message": "Comment crÃ©er un compte ?",
    "user_id": "user123",
    "use_llm": true
  }
  ```

- `GET /` - Interface web

### Endpoints LLM

- `GET /llm/status` - VÃ©rifier le statut des providers LLM
- `POST /llm/switch/{provider}` - Changer de provider LLM

### Endpoints Admin

- `GET /admin/new-questions` - Voir les nouvelles questions
- `POST /admin/add-to-faq/{question_index}` - Ajouter une question Ã  la FAQ

### Endpoints SystÃ¨me

- `GET /health` - VÃ©rification de santÃ©
- `GET /metrics` - Statistiques du systÃ¨me

## ğŸ§ª Tests

Lancer les tests :

```bash
pytest test/test_retrieval.py -v
```

## ğŸ¯ Fonctionnement

1. **Phase Retrieval** : 
   - L'utilisateur envoie une question
   - Le systÃ¨me recherche dans la FAQ par similaritÃ© sÃ©mantique
   - Calcul du score de confiance

2. **Phase Generation** :
   - Si confidence < 0.45 : Utiliser le LLM pour rÃ©pondre Ã  une nouvelle question
   - Si confidence >= 0.45 : Utiliser le LLM pour amÃ©liorer la rÃ©ponse de la FAQ (si activÃ©)
   - Si pas de LLM : Retourner la rÃ©ponse directement de la FAQ

3. **MÃ©moire** :
   - Sauvegarder l'historique de conversation par user_id
   - Conserver un maximum de 5 messages rÃ©cents (configurable)

## ğŸ“Š MÃ©triques et Monitoring

AccÃ©der Ã  `/metrics` pour voir :
- Nombre de requÃªtes traitÃ©es
- Distribution des scores de confiance
- Taux de rÃ©ussite du cache
- Nombre d'utilisateurs actifs
- Statut des providers LLM

## ğŸ”’ SÃ©curitÃ©

- âœ… Les clÃ©s API sont stockÃ©es dans `.env` (pas commitÃ©es dans git)
- âœ… CORS configurÃ© (peut Ãªtre restreint aux origines en production)
- âœ… Validation des entrÃ©es avec Pydantic
- âš ï¸ **Production** : Devrait ajouter authentication/authorization

## ğŸš€ DÃ©ploiement

### Avec Docker (Ã  venir)

```bash
docker build -t chatbot-rag .
docker run -p 8000:8000 --env-file .env chatbot-rag
```

### Avec Gunicorn

```bash
pip install gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
```

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Committer les changements (`git commit -m 'Add some AmazingFeature'`)
4. Pousser vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est publiÃ© sous licence MIT.

## ğŸ™ Remerciements

- [FastAPI](https://fastapi.tiangolo.com/)
- [SentenceTransformers](https://www.sbert.net/)
- [Groq](https://groq.com/) - API LLM gratuite
- [Uvicorn](https://www.uvicorn.org/)

## ğŸ“ Support

Si vous rencontrez un problÃ¨me :
1. VÃ©rifier que le fichier `.env` contient `GROQ_API_KEY` correctement
2. VÃ©rifier les logs dans la console
3. Lancer le health check : `GET /health`
4. VÃ©rifier le statut LLM : `GET /llm/status`

## ğŸ“ Pour en savoir plus

- [Pattern RAG](https://www.promptingguide.ai/techniques/rag)
- [Sentence Transformers](https://www.sbert.net/docs/usage/semantic_textual_similarity.html)
- [Documentation FastAPI](https://fastapi.tiangolo.com/)

---

Fait avec â¤ï¸ en utilisant Python & FastAPI
